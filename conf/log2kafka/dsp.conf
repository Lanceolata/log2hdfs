# librdkafka configure
[kafka]
client.id = log2hdfs_log2kafka
max.in.flight.requests.per.connection = 10
queue.buffering.max.messages = 2000
queue.buffering.max.ms = 200
message.send.max.retries = 3
batch.num.messages = 3000

# old kafka
metadata.broker.list = 192.168.145.201:9092,192.168.145.202:9092,192.168.145.203:9092,192.168.145.204:9092,192.168.145.205:9092,192.168.145.206:9092,192.168.145.207:9092,192.168.145.208:9092,192.168.145.209:9092,192.168.145.210:9092
broker.version.fallback = 0.8.2

# new kafka
#metadata.broker.list = 192.168.145.216:9092,192.168.145.218:9092,192.168.145.221:9092,192.168.145.222:9092,192.168.145.223:9092,192.168.145.224:9092,192.168.145.225:9092,192.168.145.226:9092,192.168.145.227:9092

# global configure
[global]
# only in global
handle.dir = dsp-remedy
handle.interval = 1800
handle.remedy = false
table.path = dsp_offset_table
table.interval = 30

# default configure
[default]
# kafka
kafka.message.timeout.ms = 60000
#kafka.compression.codec = snappy

remedy = 0

# log2kafka produce
batch.num = 200
poll.timeout = 300
poll.messages = 500

# topic configure
[bid]
dirs = /data/ef-logs/bid

[unbid]
dirs = /data/ef-logs/unbid

[pdb-bid]
dirs = /data/ef-logs/pdb-bid

[pdb-unbid]
dirs = /data/ef-logs/pdb-unbid
