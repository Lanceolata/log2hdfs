# hdfs configuration
[hdfs]
type = command
namenode = hdfs://hadoopcluster
port = 8020
user = data-infra
put = hadoop fs -put
append = hadoop fs -appendToFile
lzo.index = hadoop jar /usr/hdp/2.4.0.0-169/hadoop/lib/hadoop-lzo-0.6.0.2.4.0.0-169.jar com.hadoop.compression.lzo.LzoIndexer

# librdkafka configuration
[kafka]
client.id = kafka2hdfs_new_orc
# internal.termination.signal = 29
group.id = kafka2hdfs_new_orc
enable.auto.commit = true
auto.commit.interval.ms = 5000
enable.auto.offset.store = true
queued.min.messages = 20000
offset.store.method = file

# old kafka
#metadata.broker.list = 192.168.145.201:9092,192.168.145.202:9092,192.168.145.203:9092,192.168.145.204:9092,192.168.145.205:9092,192.168.145.206:9092,192.168.145.207:9092,192.168.145.208:9092,192.168.145.209:9092,192.168.145.210:9092
#broker.version.fallback = 0.8.2

# new kafka
metadata.broker.list = 192.168.145.216:9092,192.168.145.217:9092,192.168.145.218:9092,192.168.145.221:9092,192.168.145.222:9092,192.168.145.223:9092,192.168.145.224:9092,192.168.145.225:9092,192.168.145.226:9092,192.168.145.227:9092

[default]
# kafka
kafka.enable.auto.commit = true
kafka.auto.commit.interval.ms = 5000
#kafka.auto.offset.reset = smallest
kafka.offset.store.method = file
kafka.offset.store.path = report-offset

root.dir = report-k2h
log.format = report
path.format = normal
consume.type = report
upload.type = orc
parallel = 4
compress.lzo = /usr/local/bin/lzop -1 -U -f --ignore-warn
compress.orc = 
compress.mv = 

consume.interval = 900
complete.interval = 180
complete.maxsize = 0
retention.seconds = 0
upload.interval = 20

[report.pv_account]
partitions = 0-17
offsets = -1000
consume.interval = 300
hdfs.path = /data/production/report/rpt_effect_base/year=%Y/month=%m/day=%d/hour=%H/%t_%Y%m%d%H%M.seq.%T.orc
compress.orc = java -cp /data/users/data-infra/kafka2hdfs/report/compress.jar com.ipinyou.compress.OrcCompress -c /data/users/data-infra/kafka2hdfs/report/schema.conf -t report.base

[report.base_reach_click]
partitions = 0-1
offsets = -1000
consume.interval = 600
hdfs.path = /data/production/report/rpt_effect_base/year=%Y/month=%m/day=%d/hour=%H/%t_%Y%m%d%H%M.seq.%T.orc
compress.orc = java -cp /data/users/data-infra/kafka2hdfs/report/compress.jar com.ipinyou.compress.OrcCompress -c /data/users/data-infra/kafka2hdfs/report/schema.conf -t report.base

[report.base_second_jump]
partitions = 0-1
offsets = -1000
consume.interval = 600
hdfs.path = /data/production/report/rpt_effect_base/year=%Y/month=%m/day=%d/hour=%H/%t_%Y%m%d%H%M.seq.%T.orc
compress.orc = java -cp /data/users/data-infra/kafka2hdfs/report/compress.jar com.ipinyou.compress.OrcCompress -c /data/users/data-infra/kafka2hdfs/report/schema.conf -t report.base

[report.base_conversion_click]
partitions = 0-1
offsets = -1000
consume.interval = 600
hdfs.path = /data/production/report/rpt_effect_base/year=%Y/month=%m/day=%d/hour=%H/%t_%Y%m%d%H%M.seq.%T.orc
compress.orc = java -cp /data/users/data-infra/kafka2hdfs/report/compress.jar com.ipinyou.compress.OrcCompress -c /data/users/data-infra/kafka2hdfs/report/schema.conf -t report.base

[report.base_conversion_imp]
partitions = 0-1
offsets = -1000
consume.interval = 600
hdfs.path = /data/production/report/rpt_effect_base/year=%Y/month=%m/day=%d/hour=%H/%t_%Y%m%d%H%M.seq.%T.orc
compress.orc = java -cp /data/users/data-infra/kafka2hdfs/report/compress.jar com.ipinyou.compress.OrcCompress -c /data/users/data-infra/kafka2hdfs/report/schema.conf -t report.base

[report.conversion_click]
partitions = 0-1
offsets = -1000
consume.interval = 600
hdfs.path = /data/production/report/ClkConv/year=%Y/month=%m/day=%d/hour=%H/%t_%Y%m%d%H%M.seq.%T.orc
compress.orc = java -cp /data/users/data-infra/kafka2hdfs/report/compress.jar com.ipinyou.compress.OrcCompress -c /data/users/data-infra/kafka2hdfs/report/schema.conf -t report.conversion_click

[report.reach_click]
partitions = 0-7
offsets = -1000
consume.interval = 600
hdfs.path = /data/production/report/reach_click/pday=%Y%m%d/phour=%H/%t_%Y%m%d%H%M.seq.%T.orc
compress.orc = java -cp /data/users/data-infra/kafka2hdfs/report/compress.jar com.ipinyou.compress.OrcCompress -c /data/users/data-infra/kafka2hdfs/report/schema.conf -t report.reach_click

[report.second_jump]
partitions = 0-1
offsets = -1000
consume.interval = 600
hdfs.path = /data/production/report/second_jump/pday=%Y%m%d/phour=%H/%t_%Y%m%d%H%M.seq.%T.orc
compress.orc = java -cp /data/users/data-infra/kafka2hdfs/report/compress.jar com.ipinyou.compress.OrcCompress -c /data/users/data-infra/kafka2hdfs/report/schema.conf -t report.second_jump

[report.pdb_analysis]
partitions = 0-19
offsets = -1000
consume.interval = 600
hdfs.path = /data/production/report/PdbAnalysis/pday=%Y%m%d/phour=%H/%t_%Y%m%d%H%M.seq.%T.orc
compress.orc = java -cp /data/users/data-infra/kafka2hdfs/report/compress.jar com.ipinyou.compress.OrcCompress -c /data/users/data-infra/kafka2hdfs/report/schema.conf -t report.pdb_analysis

[report.stats_service]
partitions = 0-1
offsets = -1000
consume.interval = 600
hdfs.path = /data/production/report/StatsService/year=%Y/month=%m/day=%d/hour=%H/%t_%Y%m%d%H%M.seq.%T.orc
compress.orc = java -cp /data/users/data-infra/kafka2hdfs/report/compress.jar com.ipinyou.compress.OrcCompress -c /data/users/data-infra/kafka2hdfs/report/schema.conf -t report.stats_service

[report.rpt_effect_pdb_return_reason]
partitions = 0-4
offsets = -1000
consume.interval = 600
hdfs.path = /data/production/report/rpt_effect_pdb_return_reason/pday=%Y%m%d/phour=%H/%t_%Y%m%d%H%M.seq.%T.orc
compress.orc = java -cp /data/users/data-infra/kafka2hdfs/report/compress.jar com.ipinyou.compress.OrcCompress -c /data/users/data-infra/kafka2hdfs/report/schema.conf -t report.rpt_effect_pdb_return_reason

[optimus.rpt_audience]
partitions = 0-1
offsets = -1000
consume.interval = 600
upload.type = text
hdfs.path = /data/production/report/rpt_audience/%Y/%m/%d/%H/%t_%Y%m%d%H%M.seq
