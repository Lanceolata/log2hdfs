[hdfs]
namenode = hdfs://hadoopcluster
port = 8020
user = data-infra

[kafka]
client.id = kafka2hdfs0
internal.termination.signal = 29
group.id = log2hdfs_kafka2hdfs_test
enable.auto.commit = true
auto.commit.interval.ms = 5000
enable.auto.offset.store = true
queued.min.messages = 12000
offset.store.method = file

# old kafka
#metadata.broker.list = 192.168.145.201:9092,192.168.145.202:9092,192.168.145.203:9092,192.168.145.204:9092,192.168.145.205:9092,192.168.145.206:9092,192.168.145.207:9092,192.168.145.208:9092,192.168.145.209:9092,192.168.145.210:9092
#broker.version.fallback = 0.8.2

# new kafka
metadata.broker.list = 192.168.145.216:9092,192.168.145.217:9092,192.168.145.218:9092,192.168.145.221:9092,192.168.145.222:9092,192.168.145.223:9092,192.168.145.224:9092,192.168.145.225:9092,192.168.145.226:9092,192.168.145.227:9092
api.version.request = true

[global]

# kafka
kafka.enable.auto.commit = true
kafka.auto.commit.interval.ms = 5000
kafka.auto.offset.reset = largest
kafka.offset.store.method = file
kafka.offset.store.path = /data/users/data-infra/kafka2hdfs/offset

# kafka2hdfs
rootdir = /data/users/data-infra/kafka2hdfs

# consume
consume.type = v6
consume.offset = KAFKA_OFFSET_STORED
consume.interval = 900
consume.complete.interval = 120

# compress
compress.type = uncompress
compress.maxsize = 5368709120
compress.interval = 20  # second
compress.lzo = /usr/local/bin/lzop -1 -U -f --ignore-warn

# upload
upload.type = text
upload.put = hadoop fs -put
upload.append = hadoop fs -appendToFile
upload.lzoindex = hadoop jar /usr/hdp/2.4.0.0-169/hadoop/lib/hadoop-lzo-0.6.0.2.4.0.0-169.jar com.hadoop.compression.lzo.LzoIndexer

[bid]
# kafka
#kafka.enable.auto.commit = true
#kafka.auto.commit.interval.ms = 5000
#kafka.auto.offset.reset = largest
#kafka.offset.store.method = file
#kafka.offset.store.path = /data/users/data-infra/kafka2hdfs/offset

# log format
log.type = v6
rootdir = /data/users/data-infra/kafka2hdfs/bid
#consume.dir = /data/users/data-infra/kafka2hdfs/k2h_log/bid
#compress.dir = /data/users/data-infra/kafka2hdfs/compress/bid
#upload.dir = /data/users/data-infra/kafka2hdfs/upload/bid

# follow configurations can safe update on runtime
interval = 15   # minute

# consume
consume.complete.interval = 120
consume.type = v6
topic.offset = KAFKA_OFFSET_STORED

# follow configurations can update on runtime
topic.names = bid;click
topic.partitions = 10,11-15,16;0-8

# compress
compress.type =
compress.command =
compress.maxsize = 
compress.interval = 20  # second

# upload
hdfs.path.format = /user/data-infra/log2hdfs-test/bid/pday=%Y%m%d/phour=%H/%t_%Y%m%d%H%M_%D.seq.%T.orc
